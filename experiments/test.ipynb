{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "import joblib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nilmlab.factories import TransformerFactory\n",
    "\n",
    "from datasources.datasource import DatasourceFactory\n",
    "from experiments import GenericExperiment\n",
    "from nilmlab.factories import EnvironmentFactory\n",
    "from nilmlab.lab import TimeSeriesLength\n",
    "from utils.logger import debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"/l/users/roberto.guillen/nilm/logs/\"\n",
    "PRETRAINED_DIR  = \"/l/users/roberto.guillen/nilm/pretrained_models/\"\n",
    "dirname = os.path.abspath('')\n",
    "\n",
    "    # Results folder\n",
    "dirname_res = os.path.join(dirname, \"../results/\")\n",
    "if not os.path.exists(dirname_res):\n",
    "    os.mkdir(dirname_res)\n",
    "\n",
    "    # Pretrained KNN weights folder\n",
    "dirname_pre = os.path.join(dirname, \"../pretrained_models/\")\n",
    "if not os.path.exists(dirname_pre):\n",
    "    os.mkdir(dirname_pre)\n",
    "\n",
    "    # Log file folder\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "    # Pretrained models folder 2\n",
    "if not os.path.exists(PRETRAINED_DIR):\n",
    "    os.mkdir(PRETRAINED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_series_length(ts_id: str = \"Hour\"):\n",
    "    if ts_id == \"10Min\":\n",
    "        ts = TimeSeriesLength.WINDOW_10_MINS\n",
    "    elif ts_id == \"Hour\":\n",
    "        ts = TimeSeriesLength.WINDOW_1_HOUR\n",
    "    elif ts_id == \"Day\":\n",
    "        ts = TimeSeriesLength.WINDOW_1_DAY\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(id: int = 0):\n",
    "    if id == 0:\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(1000,), learning_rate='adaptive', solver='adam',early_stopping=True,validation_fraction=0.2)\n",
    "    elif id == 1:\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(2000, 100, 100), learning_rate='adaptive', solver='adam',early_stopping=True,validation_fraction=0.2)\n",
    "    elif id == 2:\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(1000, 100), learning_rate='adaptive', solver='adam',early_stopping=True,validation_fraction=0.2)\n",
    "    elif id == 3:\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(100, 100, 100), learning_rate='adaptive', solver='adam',early_stopping=True,validation_fraction=0.2)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasource(datasource_ix = 0): # 0,false,ukdale    / 1,true,redd\n",
    "    if datasource_ix:\n",
    "        datasource_name = 'redd'\n",
    "        datasource = DatasourceFactory.create_redd_datasource()\n",
    "        appliances = [\n",
    "            'unknown', 'electric oven','sockets', 'electric space heater', 'microwave', \n",
    "            'washer dryer', 'light', 'electric stove', 'dish washer', 'fridge'\n",
    "        ]\n",
    "        \n",
    "        env = EnvironmentFactory.create_env_single_building(\n",
    "            datasource=datasource,\n",
    "            building=1,\n",
    "            sample_period=6,\n",
    "            train_year=\"2011-2011\",\n",
    "            train_start_date=\"4-1-2011\",\n",
    "            train_end_date=\"4-30-2011\",\n",
    "            test_year=\"2011\",\n",
    "            test_start_date=\"5-1-2011\",\n",
    "            test_end_date=\"5-2-2011\",\n",
    "            appliances=appliances\n",
    "        )\n",
    "    else:\n",
    "        datasource_name = 'ukdale' # Review name change\n",
    "        datasource = DatasourceFactory.create_uk_dale_datasource()\n",
    "        appliances = [\n",
    "            'microwave', 'dish washer', 'fridge', 'kettle', 'washer dryer',\n",
    "            'toaster', 'television'\n",
    "        ]\n",
    "        env = EnvironmentFactory.create_env_single_building(\n",
    "            datasource=DatasourceFactory.create_uk_dale_datasource(),\n",
    "            building=1,\n",
    "            sample_period=6,\n",
    "            train_year=\"2013-2013\",\n",
    "            train_start_date=\"6-2-2013\",\n",
    "            train_end_date=\"6-3-2013\",\n",
    "            # train_year=\"2013-2014\",\n",
    "            # train_start_date=\"4-12-2013\",\n",
    "            # train_end_date=\"6-01-2014\",\n",
    "            test_year=\"2014\",\n",
    "            test_start_date=\"6-2-2014\",\n",
    "            test_end_date=\"6-3-2014\",\n",
    "            # test_start_date=\"6-2-2014\",\n",
    "            # test_end_date=\"12-30-2014\",\n",
    "            appliances=appliances\n",
    "        )\n",
    "    experiment = GenericExperiment(env)\n",
    "    return datasource_name, appliances, experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Working on setting 99999999  PATH:  /l/users/roberto.guillen/nilm/logs/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0.log\n",
      "/l/users/roberto.guillen/nilm/pretrained_models/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0_weight.pkl /l/users/roberto.guillen/nilm/pretrained_models/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0_emb.pkl /home/roberto.guillen/Documents/multi-nilm/experiments/../results/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0.csv\n",
      "INFO: Reading data from specified meters. \n",
      "-Building: 1\n",
      "-Appliances ['microwave', 'dish washer', 'fridge', 'kettle', 'washer dryer', 'toaster', 'television']\n",
      "DEBUG:  read_selected_appliances ['microwave', 'dish washer', 'fridge', 'kettle', 'washer dryer', 'toaster', 'television'], 1, 6-2-2013, 6-3-2013, True\n",
      "TIMING: NILMTK select using appliances: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 01:44:38.115 | DEBUG    | nilmlab.lab:setup_one_building:576 - Length of data of all loaded meters 14400\n",
      "/home/roberto.guillen/.conda/envs/nilmtk-env/lib/python3.7/site-packages/nilmtk/elecmeter.py:190: RuntimeWarning: Multiple appliances are associated with meter {} but none are marked as the dominant appliance. Hence returning the first appliance in the list.\n",
      "  ' returning the first appliance in the list.', RuntimeWarning)\n",
      "2022-07-01 01:44:38.118 | DEBUG    | nilmlab.lab:setup_one_building:578 - Length of data of all loaded meters 14400\n",
      "2022-07-01 01:44:38.119 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter washer dryer, \n",
      "labels2id[col] (5, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=5, building=1, dataset='UK-DALE', appliances=[Appliance(type='washer dryer', instance=1), Appliance(type='washer dryer', instance=2)])\n",
      "2022-07-01 01:44:38.119 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) washer dryer - [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: NILMTK converting specified appliances to dataframe: 0.31\n",
      "DEBUG: Length of data of read_selected_appliances 14400\n",
      "INFO: Df columns before normalization Index([ (5, 1, 'UK-DALE'), (11, 1, 'UK-DALE'),  (7, 1, 'UK-DALE'),\n",
      "       (54, 1, 'UK-DALE'), (13, 1, 'UK-DALE'), (10, 1, 'UK-DALE'),\n",
      "        (6, 1, 'UK-DALE'), (12, 1, 'UK-DALE')],\n",
      "      dtype='object')\n",
      "INFO: Labels before normalization ['Washer dryer', 'Toaster', 'Television', 'Site meter', 'Microwave', 'Kettle', 'Dish washer', 'Fridge freezer']\n",
      "INFO: washer dryer ~ Washer dryer (100%)\n",
      "INFO: toaster ~ Toaster (100%)\n",
      "INFO: television ~ Television (100%)\n",
      "INFO: microwave ~ Microwave (100%)\n",
      "INFO: kettle ~ Kettle (100%)\n",
      "INFO: dish washer ~ Dish washer (100%)\n",
      "INFO: fridge ~ Fridge freezer (100%)\n",
      "INFO: Normalized labels ['washer dryer', 'toaster', 'television', 'Site meter', 'microwave', 'kettle', 'dish washer', 'fridge']\n",
      "INFO: Meters that have been loaded (all_df.columns):\n",
      "Index(['washer dryer', 'toaster', 'television', 'Site meter', 'microwave',\n",
      "       'kettle', 'dish washer', 'fridge'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roberto.guillen/.conda/envs/nilmtk-env/lib/python3.7/site-packages/numba/core/typed_passes.py:330: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../datasources/labels_factory.py\", line 95:\n",
      "# 1 if appliance is on at that time and 0 if appliance is off at that time (usually 6 secs)\n",
      "def create_labels(array, threshold):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "2022-07-01 01:44:38.600 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter toaster, \n",
      "labels2id[col] (11, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=11, building=1, dataset='UK-DALE', appliances=[Appliance(type='toaster', instance=1), Appliance(type='kitchen aid', instance=1), Appliance(type='food processor', instance=2)])\n",
      "2022-07-01 01:44:38.602 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) toaster - [0. 0. 0. ... 0. 0. 0.]\n",
      "2022-07-01 01:44:38.603 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter television, \n",
      "labels2id[col] (7, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=7, building=1, dataset='UK-DALE', appliances=[Appliance(type='television', instance=1)])\n",
      "2022-07-01 01:44:38.604 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) television - [1. 1. 1. ... 1. 1. 1.]\n",
      "2022-07-01 01:44:38.605 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter Site meter, \n",
      "labels2id[col] (54, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=54, building=1, dataset='UK-DALE', site_meter, appliances=[Appliance(type='immersion heater', instance=1), Appliance(type='water pump', instance=1), Appliance(type='security alarm', instance=1), Appliance(type='fan', instance=2), Appliance(type='drill', instance=1), Appliance(type='laptop computer', instance=2)])\n",
      "2022-07-01 01:44:38.607 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:35 - Skipping Site meter - [232.31666565 232.1416626  232.30334473 ...  87.45833588  87.32499695\n",
      "  87.26667023]\n",
      "2022-07-01 01:44:38.607 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter microwave, \n",
      "labels2id[col] (13, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=13, building=1, dataset='UK-DALE', appliances=[Appliance(type='microwave', instance=1)])\n",
      "2022-07-01 01:44:38.608 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) microwave - [1. 1. 1. ... 1. 1. 1.]\n",
      "2022-07-01 01:44:38.609 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter kettle, \n",
      "labels2id[col] (10, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=10, building=1, dataset='UK-DALE', appliances=[Appliance(type='kettle', instance=1), Appliance(type='food processor', instance=1), Appliance(type='toasted sandwich maker', instance=1)])\n",
      "2022-07-01 01:44:38.611 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) kettle - [0. 1. 1. ... 1. 1. 0.]\n",
      "2022-07-01 01:44:38.612 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter dish washer, \n",
      "labels2id[col] (6, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=6, building=1, dataset='UK-DALE', appliances=[Appliance(type='dish washer', instance=1)])\n",
      "2022-07-01 01:44:38.614 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) dish washer - [0. 1. 1. ... 1. 1. 0.]\n",
      "2022-07-01 01:44:38.615 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter fridge, \n",
      "labels2id[col] (12, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=12, building=1, dataset='UK-DALE', appliances=[Appliance(type='fridge freezer', instance=1)])\n",
      "2022-07-01 01:44:38.617 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) fridge - [ 0. 83. 83. ...  0.  0.  0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: Create multilabels from meters 0.5\n",
      "INFO: Reading data from specified meters. \n",
      "-Building: 1\n",
      "-Appliances ['microwave', 'dish washer', 'fridge', 'kettle', 'washer dryer', 'toaster', 'television']\n",
      "DEBUG:  read_selected_appliances ['microwave', 'dish washer', 'fridge', 'kettle', 'washer dryer', 'toaster', 'television'], 1, 6-2-2014, 6-3-2014, True\n",
      "TIMING: NILMTK select using appliances: 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 01:44:39.012 | DEBUG    | nilmlab.lab:setup_one_building:576 - Length of data of all loaded meters 14400\n",
      "/home/roberto.guillen/.conda/envs/nilmtk-env/lib/python3.7/site-packages/nilmtk/elecmeter.py:190: RuntimeWarning: Multiple appliances are associated with meter {} but none are marked as the dominant appliance. Hence returning the first appliance in the list.\n",
      "  ' returning the first appliance in the list.', RuntimeWarning)\n",
      "2022-07-01 01:44:39.014 | DEBUG    | nilmlab.lab:setup_one_building:578 - Length of data of all loaded meters 14400\n",
      "2022-07-01 01:44:39.015 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter washer dryer, \n",
      "labels2id[col] (5, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=5, building=1, dataset='UK-DALE', appliances=[Appliance(type='washer dryer', instance=1), Appliance(type='washer dryer', instance=2)])\n",
      "2022-07-01 01:44:39.016 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) washer dryer - [0. 0. 0. ... 0. 0. 0.]\n",
      "2022-07-01 01:44:39.017 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter toaster, \n",
      "labels2id[col] (11, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=11, building=1, dataset='UK-DALE', appliances=[Appliance(type='toaster', instance=1), Appliance(type='kitchen aid', instance=1), Appliance(type='food processor', instance=2)])\n",
      "2022-07-01 01:44:39.018 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) toaster - [0. 0. 0. ... 0. 0. 0.]\n",
      "2022-07-01 01:44:39.019 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter television, \n",
      "labels2id[col] (7, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=7, building=1, dataset='UK-DALE', appliances=[Appliance(type='television', instance=1)])\n",
      "2022-07-01 01:44:39.020 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) television - [1. 1. 1. ... 1. 1. 1.]\n",
      "2022-07-01 01:44:39.020 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter Site meter, \n",
      "labels2id[col] (54, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=54, building=1, dataset='UK-DALE', site_meter, appliances=[Appliance(type='immersion heater', instance=1), Appliance(type='water pump', instance=1), Appliance(type='security alarm', instance=1), Appliance(type='fan', instance=2), Appliance(type='drill', instance=1), Appliance(type='laptop computer', instance=2)])\n",
      "2022-07-01 01:44:39.022 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:35 - Skipping Site meter - [ 89.05166626  88.82500458  88.80000305 ... 184.76333618 184.27999878\n",
      " 184.5083313 ]\n",
      "2022-07-01 01:44:39.022 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter microwave, \n",
      "labels2id[col] (13, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=13, building=1, dataset='UK-DALE', appliances=[Appliance(type='microwave', instance=1)])\n",
      "2022-07-01 01:44:39.023 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) microwave - [1. 1. 1. ... 1. 1. 1.]\n",
      "2022-07-01 01:44:39.024 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter kettle, \n",
      "labels2id[col] (10, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=10, building=1, dataset='UK-DALE', appliances=[Appliance(type='kettle', instance=1), Appliance(type='food processor', instance=1), Appliance(type='toasted sandwich maker', instance=1)])\n",
      "2022-07-01 01:44:39.025 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) kettle - [0. 1. 1. ... 1. 1. 1.]\n",
      "2022-07-01 01:44:39.026 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter dish washer, \n",
      "labels2id[col] (6, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=6, building=1, dataset='UK-DALE', appliances=[Appliance(type='dish washer', instance=1)])\n",
      "2022-07-01 01:44:39.030 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) dish washer - [1. 1. 1. ... 1. 1. 1.]\n",
      "2022-07-01 01:44:39.031 | INFO     | datasources.labels_factory:create_multilabels_from_meters:28 - Creating multilabels from meter fridge, \n",
      "labels2id[col] (12, 1, 'UK-DALE')\n",
      "metergroup[labels2id[col]] ElecMeter(instance=12, building=1, dataset='UK-DALE', appliances=[Appliance(type='fridge freezer', instance=1)])\n",
      "2022-07-01 01:44:39.031 | DEBUG    | datasources.labels_factory:create_multilabels_from_meters:37 - meters[col].values.astype(float) fridge - [ 0.  0.  0. ... 87. 87. 86.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: NILMTK converting specified appliances to dataframe: 0.24\n",
      "DEBUG: Length of data of read_selected_appliances 14400\n",
      "INFO: Df columns before normalization Index([ (5, 1, 'UK-DALE'), (11, 1, 'UK-DALE'),  (7, 1, 'UK-DALE'),\n",
      "       (54, 1, 'UK-DALE'), (13, 1, 'UK-DALE'), (10, 1, 'UK-DALE'),\n",
      "        (6, 1, 'UK-DALE'), (12, 1, 'UK-DALE')],\n",
      "      dtype='object')\n",
      "INFO: Labels before normalization ['Washer dryer', 'Toaster', 'Television', 'Site meter', 'Microwave', 'Kettle', 'Dish washer', 'Fridge freezer']\n",
      "INFO: washer dryer ~ Washer dryer (100%)\n",
      "INFO: toaster ~ Toaster (100%)\n",
      "INFO: television ~ Television (100%)\n",
      "INFO: microwave ~ Microwave (100%)\n",
      "INFO: kettle ~ Kettle (100%)\n",
      "INFO: dish washer ~ Dish washer (100%)\n",
      "INFO: fridge ~ Fridge freezer (100%)\n",
      "INFO: Normalized labels ['washer dryer', 'toaster', 'television', 'Site meter', 'microwave', 'kettle', 'dish washer', 'fridge']\n",
      "INFO: Meters that have been loaded (all_df.columns):\n",
      "Index(['washer dryer', 'toaster', 'television', 'Site meter', 'microwave',\n",
      "       'kettle', 'dish washer', 'fridge'],\n",
      "      dtype='object')\n",
      "TIMING: Create multilabels from meters 0.02\n",
      "INFO: Prepossessing before training...\n",
      "DEBUG: Type of transformer TransformerType.transform_and_approximate\n",
      "DEBUG: MySignal2Vec.get_token_sequence: Transform to discrete space.\n",
      "DEBUG: Memory: svmem(total=270393212928, available=234199977984, percent=13.4, used=33878278144, free=224470319104, active=16464904192, inactive=9390972928, buffers=28012544, cached=12016603136, shared=28925952, slab=16780673024)\n",
      "DEBUG: Spliting data into 1 parts for memory efficient clustering\n",
      "MEMORY: Time series 0.0550384521484375 MB\n",
      "MEMORY: Data chunks 9.918212890625e-05 MB\n",
      "DEBUG: Best BIC -63395.09950149971\n",
      "TIMING: MySignal2Vec.get_token_sequence: Getting best GMM : 0.14\n",
      "DEBUG: MySignal2Vec.train_quantization_clf: Train.\n",
      "DEBUG: Quantization N tokens 8\n",
      "DEBUG: MySignal2Vec.train_quantization_clf: saving checkpoint.\n",
      "DEBUG: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2365123/571907626.py\", line 67, in <module>\n",
      "    experiment.run()\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/experiments/experiments.py\", line 129, in run\n",
      "    preprocess_train_time, fit_time = self.env.train(self.train_appliances)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/lab.py\", line 634, in train\n",
      "    data, target = self._preprocess(self.train_df, self.train_labels_df, appliances, self.get_ts_len(), raw_data)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/lab.py\", line 693, in _preprocess\n",
      "    data = self.get_features(data_df, representation_type)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/lab.py\", line 459, in get_features\n",
      "    data = self.ts_transformer.transform(data)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/tstransformers.py\", line 93, in transform\n",
      "    self.train_quantization_clf(n_neighbors=n_clusters, X=scaled_series, y=token_sequence)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/tstransformers.py\", line 219, in train_quantization_clf\n",
      "    joblib.dump(self.quant_clf,file_name)\n",
      "  File \"/home/roberto.guillen/.conda/envs/nilmtk-env/lib/python3.7/site-packages/joblib/numpy_pickle.py\", line 481, in dump\n",
      "    with open(filename, 'wb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/roberto.guillen/Documents/multi-nilm/experiments/pretrained_models/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0_weight.pkl'\n",
      "\n",
      "DEBUG: Failed for MYSIGNAL2VEC_Build\n",
      "DEBUG: [Errno 2] No such file or directory: '/home/roberto.guillen/Documents/multi-nilm/experiments/pretrained_models/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0_weight.pkl'\n",
      "INFO: Prepossessing before training...\n",
      "DEBUG: Type of transformer TransformerType.transform_and_approximate\n",
      "DEBUG: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2365123/571907626.py\", line 67, in <module>\n",
      "    experiment.run()\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/experiments/experiments.py\", line 129, in run\n",
      "    preprocess_train_time, fit_time = self.env.train(self.train_appliances)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/lab.py\", line 634, in train\n",
      "    data, target = self._preprocess(self.train_df, self.train_labels_df, appliances, self.get_ts_len(), raw_data)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/lab.py\", line 693, in _preprocess\n",
      "    data = self.get_features(data_df, representation_type)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/lab.py\", line 459, in get_features\n",
      "    data = self.ts_transformer.transform(data)\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/tstransformers.py\", line 326, in transform\n",
      "    self.load_weights()\n",
      "  File \"/home/roberto.guillen/Documents/multi-nilm/nilmlab/tstransformers.py\", line 318, in load_weights\n",
      "    self.clf = joblib.load(self.classifier_path)\n",
      "  File \"/home/roberto.guillen/.conda/envs/nilmtk-env/lib/python3.7/site-packages/joblib/numpy_pickle.py\", line 579, in load\n",
      "    with open(filename, 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/l/users/roberto.guillen/nilm/pretrained_models/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0_weight.pkl'\n",
      "\n",
      "DEBUG: Failed for MYSIGNAL2VEC_Infer\n",
      "DEBUG: [Errno 2] No such file or directory: '/l/users/roberto.guillen/nilm/pretrained_models/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0_weight.pkl'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/roberto.guillen/Documents/multi-nilm/experiments/../results/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0.csv' does not exist: b'/home/roberto.guillen/Documents/multi-nilm/experiments/../results/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2365123/571907626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# TODO change how saving is handled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nilmtk-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nilmtk-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nilmtk-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nilmtk-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nilmtk-env/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/roberto.guillen/Documents/multi-nilm/experiments/../results/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0.csv' does not exist: b'/home/roberto.guillen/Documents/multi-nilm/experiments/../results/ukdale/mys2v_components_8_tsLength_Day_numRepVec_1_classifier_type_0.csv'"
     ]
    }
   ],
   "source": [
    "datasource_ix = 0\n",
    "datasource_name = \"ukdale\"\n",
    "i = 99999999\n",
    "components = 8\n",
    "ts_length = \"Day\"\n",
    "num_rep_vec = 1\n",
    "classifier_type = 0 \n",
    "\n",
    "# Review if log file exists\n",
    "exp_name = \"mys2v_components_%d_tsLength_%s_numRepVec_%d_classifier_type_%d\"%(components, ts_length, num_rep_vec,classifier_type) \n",
    "log_file_name = datasource_name + \"/\" + exp_name + \".log\" \n",
    "log_file_path = os.path.join(LOG_DIR, log_file_name)\n",
    "if os.path.exists(log_file_path):\n",
    "    # Skip if exists\n",
    "    pass\n",
    "# Create file\n",
    "pickle.dump({},open(log_file_path,\"wb\"))\n",
    "print(\"\\n Working on setting\", i, \" PATH: \" ,log_file_path)\n",
    "\n",
    "# Names for files \n",
    "PRETRAINED_DIR = PRETRAINED_DIR + datasource_name + \"/\" \n",
    "dirname_res = dirname_res + datasource_name + \"/\" \n",
    "mys2v_knn_weights = os.path.join(PRETRAINED_DIR, f'{exp_name}_weight.pkl')\n",
    "mys2v_embedding = os.path.join(PRETRAINED_DIR, f'{exp_name}_emb.pkl')\n",
    "results_file_name = os.path.join(dirname_res, f'{exp_name}.csv')\n",
    "print(mys2v_knn_weights,mys2v_embedding,results_file_name)\n",
    "\n",
    "# Other params\n",
    "window_size = 10\n",
    "window_step = 1\n",
    "epochs  = 2\n",
    "\n",
    "models =  infer_mysignal2vec_experiment = {\n",
    "    'MYSIGNAL2VEC_Build' : {\n",
    "        'CLF_MODELS' : [ \n",
    "            get_classifier(classifier_type),\n",
    "        ],\n",
    "        'TRANSFORMER_MODELS': [\n",
    "            TransformerFactory.build_mysignal2vec_train(num_rep_vec, window_size, window_step, components, components, epochs, exp_name),\n",
    "        ]\n",
    "    },\n",
    "    'MYSIGNAL2VEC_Infer' : {\n",
    "        'CLF_MODELS' : [ \n",
    "            get_classifier(classifier_type),\n",
    "        ],\n",
    "        'TRANSFORMER_MODELS': [\n",
    "            TransformerFactory.build_mysignal2vec_infer(mys2v_knn_weights, mys2v_embedding, num_rep_vec),\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "datasource_name, appliances, experiment = get_datasource(datasource_ix)\n",
    "for k in models.keys():      \n",
    "    experiment.setup_running_params(\n",
    "        transformer_models=models[k]['TRANSFORMER_MODELS'],\n",
    "        classifier_models=models[k]['CLF_MODELS'],\n",
    "        train_appliances=appliances,\n",
    "        test_appliances=appliances,\n",
    "        ts_len=get_time_series_length(ts_length),\n",
    "        repeat=1\n",
    "    )\n",
    "\n",
    "    experiment.set_checkpoint_file(results_file_name)\n",
    "    tb = \"No error\"\n",
    "    \n",
    "    try:\n",
    "        experiment.run()\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        debug(tb)\n",
    "        debug(f\"Failed for {k}\")\n",
    "        debug(f\"{e}\")\n",
    "    \n",
    "# TODO change how saving is handled\n",
    "df = pd.read_csv(results_file_name)\n",
    "joblib.dump(df, log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_ix = 0\n",
    "datasource_name = \"ukdale\"\n",
    "\n",
    "for i, (components, ts_length, num_rep_vec,classifier_type) in enumerate(product([2**p for p in range(2, 9)], [\"10Min\",\"Hour\",\"Day\"], [1,2,4,8],[0,1,2,3])):\n",
    "    # Review if log file exists\n",
    "    exp_name = \"mys2v_components_%d_tsLength_%s_numRepVec_%d_classifier_type_%d\"%(components, ts_length, num_rep_vec,classifier_type) \n",
    "    log_file_name = datasource_name + \"/\" + exp_name + \".log\" \n",
    "    log_file_path = os.path.join(LOG_DIR, log_file_name)\n",
    "    if os.path.exists(log_file_path):\n",
    "        # Skip if exists\n",
    "        continue\n",
    "    # Create file\n",
    "    pickle.dump({},open(log_file_path,\"wb\"))\n",
    "    print(\"\\n Working on setting\", i, \" PATH: \" ,log_file_path)\n",
    "    \n",
    "    # Names for files \n",
    "    mys2v_knn_weights = os.path.join(PRETRAINED_DIR, f'{exp_name}_weight.pkl')\n",
    "    mys2v_embedding = os.path.join(PRETRAINED_DIR, f'{exp_name}_emb.pkl')\n",
    "    results_file_name = os.path.join(dirname_res, f'{exp_name}.csv')\n",
    "    print(mys2v_knn_weights,mys2v_embedding,results_file_name)\n",
    "    break\n",
    "    # Other params\n",
    "    window_size = 10\n",
    "    window_step = 1\n",
    "    epochs  = 2\n",
    "\n",
    "\n",
    "    models =  infer_mysignal2vec_experiment = {\n",
    "        'MYSIGNAL2VEC_Build' : {\n",
    "            'CLF_MODELS' : [ \n",
    "                get_classifier(classifier_type),\n",
    "            ],\n",
    "            'TRANSFORMER_MODELS': [\n",
    "                TransformerFactory.build_mysignal2vec_train(num_rep_vec, window_size, window_step, components, components, epochs, exp_name),\n",
    "            ]\n",
    "        },\n",
    "        'MYSIGNAL2VEC_Infer' : {\n",
    "            'CLF_MODELS' : [ \n",
    "                get_classifier(classifier_type),\n",
    "            ],\n",
    "            'TRANSFORMER_MODELS': [\n",
    "            TransformerFactory.build_mysignal2vec_infer(mys2v_knn_weights, mys2v_embedding, num_rep_vec),\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    datasource_name, appliances, experiment = get_datasource(datasource_ix)\n",
    "    for k in models.keys():      \n",
    "        experiment.setup_running_params(\n",
    "            transformer_models=models[k]['TRANSFORMER_MODELS'],\n",
    "            classifier_models=models[k]['CLF_MODELS'],\n",
    "            train_appliances=appliances,\n",
    "            test_appliances=appliances,\n",
    "            ts_len=get_time_series_length(ts_length),\n",
    "            repeat=1\n",
    "        )\n",
    "\n",
    "        experiment.set_checkpoint_file(results_file_name)\n",
    "        tb = \"No error\"\n",
    "        \n",
    "        try:\n",
    "            experiment.run()\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc()\n",
    "            debug(tb)\n",
    "            debug(f\"Failed for {k}\")\n",
    "            debug(f\"{e}\")\n",
    "    # TODO change how saving is handled\n",
    "    df = pd.read_csv(results_file_name)\n",
    "    joblib.dump(df, log_file_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('nilmtk-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2798ecf838f76291c7848af8fe987a15a8c1ed6292b025a28164137c09ab617"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
